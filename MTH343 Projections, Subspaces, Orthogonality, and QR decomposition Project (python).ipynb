{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# Kira Novitchkova-Burbank"],"metadata":{"id":"AEaTQpXNo0y8"}},{"cell_type":"markdown","source":["# Section 3: Projections, Subspaces, Orthogonality, and QR decomposition\n","\n","In lecture you have been discussing subspaces and the notion of orthogonality. Generating orthogonal subspaces or an orthonormal basis for matrices can be a powerful numerical tool.\n","\n","In this section we will explore this idea of orthogonality and how to use it to describe matrices and solve least squares."],"metadata":{"id":"1s7Anw7c34pD"}},{"cell_type":"markdown","source":["## Using QR for least squares\n","\n","We can use least squares and QR to attempt to classify handwritten digits from the MNIST dataset. This is essentially a single layer perceptron with no activation function. We will use tensorflow to load the data since it has a nice loader to numpy."],"metadata":{"id":"U19FClQKBRgJ"}},{"cell_type":"code","source":["import tensorflow as tf\n","import numpy as np\n","import scipy\n","(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n","\n","print(x_train.shape)\n","print(y_train.shape)"],"metadata":{"id":"0Ifc767FYcc-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668231117930,"user_tz":480,"elapsed":460,"user":{"displayName":"Kira Burbank","userId":"10437625992165419595"}},"outputId":"fdf523f9-a5bd-466d-9330-30ff62fd78d4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(60000, 28, 28)\n","(60000,)\n"]}]},{"cell_type":"markdown","source":["We see here that each training example is a 28x28 image. We want each example as a single vector so let's flatten that to shape to make a large data matrix."],"metadata":{"id":"xHIAgd8HaHj9"}},{"cell_type":"code","source":["x_train = x_train.reshape((60000, 784))\n","bias = np.ones((60000, 1))\n","#A = x_train\n","x_train = np.concatenate((bias, x_train), axis=1)"],"metadata":{"id":"_k2IRCD_Zr0k"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's call the data matrix $A$ and the labels $b$. There likely isn't a solution to the system $Ax = b$ since the matrix $A$ has many less columns than rows. This means we want to solve $\\min_{x}||Ax - b||_{l_2}$, which is the least squares problem. Let's think about how we can do this using $QR$.\n","\n","Fact:\n","1. Because $Q$ is orthonormal, it doesn't change the norm of any vector.\n","\n","Proof:\n","$$||Qy||^2_2 = (Qy)^T (Qy) = y^TQ^TQy = y^ty = ||y||_2^2$$\n","\n","2. So that means we can transform our minimization problem to:\n","\\begin{align}\n","&=\\min_x ||Ax - b||_2\\\\\n","&=\\min_x ||Q^T(Ax - b)||_2\\\\\n","&=\\min_x ||Q^T(QRx - b)||_2\\\\\n","&=\\min_x ||Rx - Q^T b||_2\n","\\end{align}\n","\n","But since $R$ is an upper triangular matrix, we know there is a solution to:\n","$$Rx = Q^T b$$\n","or\n","$$x = R^{-1} Q^T b$$\n","which would make the result of the minimization $0$."],"metadata":{"id":"Z0aayBooas_-"}},{"cell_type":"code","source":["Q, R = scipy.linalg.qr(x_train, mode='economic')\n","\n","np.linalg.matrix_rank(R)"],"metadata":{"id":"i8tHJGQIgNNp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668231148126,"user_tz":480,"elapsed":9569,"user":{"displayName":"Kira Burbank","userId":"10437625992165419595"}},"outputId":"941fbcd9-ad35-4957-f34e-8329aa3fbfb4"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["713"]},"metadata":{},"execution_count":29}]},{"cell_type":"markdown","source":["Notice the above technique only works if $R$ is invertible, or full rank. $R$ will only be full rank if the original data, $A$, has linearly independent columns. Often times there are linear dependencies in the dataset, meaning we have to take a slightly different approach to the $QR$ for least squares.\n","\n","This process is known as *rank deficient least squares* and requires a modified $QR$ which permutes the $A$ matrix so that the diagonal or $R$ is not increasing. If you're interested, [here is a formal description of this algorithm using householder transformations](https://www.math.usm.edu/lambers/mat610/sum10/lecture11.pdf).\n","\n","For our case, `scipy` offers a `pivoting` argument flag that does this for us."],"metadata":{"id":"p8XVVNsIavGQ"}},{"cell_type":"code","source":["Q, R_p, p = scipy.linalg.qr(x_train, mode='economic', pivoting=True)"],"metadata":{"id":"AKBn4nB-cc8P"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This version gives us a $Q$ like before but the $R_p$ is provided in the form of:\n","$$R_p = \\begin{bmatrix}\n","R & S\\\\\n","0 & 0\n","\\end{bmatrix} $$\n","Where the $R$ now is a true upper triangular. The provided $p$ is the pivots required to $A$ to satisfy:\n","$$A \\Pi = Q R_p$$\n","With $\\Pi$ being the permutation matrix created from $p$. We can create that permutation matrix with `np.eye(size)[:,p]`.\n","\n","We checked previously that the rank of our R matrix is 713 but let's check again with the pivoted R by looking for the first 0 in the diagonal."],"metadata":{"id":"0O6I9K9ohpRw"}},{"cell_type":"code","source":["rank = np.argmax(np.absolute(np.diag(R_p)) < 1e-6)\n","rank"],"metadata":{"id":"dUmNC7etgBpO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668231178188,"user_tz":480,"elapsed":171,"user":{"displayName":"Kira Burbank","userId":"10437625992165419595"}},"outputId":"c7d87214-d717-4d44-a4c8-4dfeb6592ffb"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["713"]},"metadata":{},"execution_count":31}]},{"cell_type":"markdown","source":["Now, one way to solve for $x$ is to slice off the part of $R$ that is all zeros and cut the bottom portion off of $Q^T b$ so that we can solve the triangular system. The resulting $x$ needs to be permuted using the pivots to get an actual solution to the least squares.\n","\n","\\begin{align}\n","||b - Ax||_2^2 &= \\left\\|b - Q\n","\\begin{bmatrix}\n","R & S\\\\\n","0 & 0\n","\\end{bmatrix}\n","\\Pi^T x \\right\\|_2^2\\\\\n","&= \\left \\|Q^T b -\n","\\begin{bmatrix}\n","R & S\\\\\n","0 & 0\n","\\end{bmatrix}\n","\\begin{bmatrix}\n","u\\\\\n","v\n","\\end{bmatrix}\n","\\right\\|_2^2\\\\\n","&=\\left\\|\n","\\begin{bmatrix}\n","c\\\\\n","d\n","\\end{bmatrix} -\n","\\begin{bmatrix}\n","Ru & Sv\\\\\n","0\n","\\end{bmatrix} \\right\\|_2^2\\\\\n","&= \\|c - Ru - Sv||_2^2 + \\|d\\|_2^2\\\\\n","\\text{where} \\quad\n","Q^Tb &=\n","\\begin{bmatrix}\n","c\\\\\n","d\n","\\end{bmatrix}, \\quad \\Pi^Tx =\n","\\begin{bmatrix}\n","u\\\\\n","v\n","\\end{bmatrix}\n","\\end{align}\n","\n","In the implementation below, I choose the simplest solution with $v=0$."],"metadata":{"id":"64IaOosYkbVu"}},{"cell_type":"code","source":["R = R_p[:rank, :rank]\n","c = Q.T[:rank,:] @ y_train\n","u = scipy.linalg.solve_triangular(R, c, lower=False)\n","v = np.zeros(785 - rank)\n","uv = np.concatenate((u,v))\n","x = np.eye(785)[:,p] @ uv\n","pred = x_train @ x\n","print(\"R:\", R)\n","print(\"pred[:10]:\", pred[:10])\n","print(\"y_train[:10]:\", y_train[:10])"],"metadata":{"id":"hJ1Et_-Jh4Fi","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668231185003,"user_tz":480,"elapsed":172,"user":{"displayName":"Kira Burbank","userId":"10437625992165419595"}},"outputId":"0bc73d95-a724-422d-daf8-71d479d64e54"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["R: [[-4.34535904e+04 -2.50719164e+04 -2.38967781e+04 ... -2.34180879e-01\n","  -1.12533854e-02 -9.27886503e-02]\n"," [ 0.00000000e+00 -3.38851685e+04 -1.04617855e+04 ... -5.33754638e-02\n","  -3.96001001e-02  6.86550896e-02]\n"," [ 0.00000000e+00  0.00000000e+00  3.08895655e+04 ... -1.99244343e-01\n","   2.81905096e-02 -4.85307884e-02]\n"," ...\n"," [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  7.95197288e+00\n","  -2.00474285e-03 -2.36763863e-03]\n"," [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n","   6.98071953e+00 -4.08163581e-04]\n"," [ 0.00000000e+00  0.00000000e+00  0.00000000e+00 ...  0.00000000e+00\n","   0.00000000e+00 -1.87255493e+00]]\n","pred[:10]: [4.19480069 1.20585109 3.19619527 2.3125187  7.70189204 4.07685415\n"," 1.62278595 3.92588536 1.84626138 4.82906635]\n","y_train[:10]: [5 0 4 1 9 2 1 3 1 4]\n"]}]},{"cell_type":"markdown","source":["We see that this is *sort of* working...\n","\n","The prediction numbers are approximately the same as the labels but with categorical labels like *digit*, getting a real number that is around the correct value isn't going to make a very good classifier.\n","\n","We can really improve this model by making 10 different binary classifiers by changing the right hand side, $y_{train}$, to be a $1$ or a $0$ depending if it is the digit we are trying to classify."],"metadata":{"id":"Gw2jJfVTvQDg"}},{"cell_type":"markdown","source":["## Exercise 3\n","\n","Here we will complete the first step to improve the least squares classifier by implementing it as a binary classifier.\n","\n","Modify `y_train` so that it *one hot encodes* the dataset. One hot encoding is where you take a vector of categorical labels and translate it to a vector for each category. Each category vector should have a $1$ if the observation is that category and a $0$ otherwise.\n","\n","### One hot encoding example:\n","$$\n","\\begin{bmatrix} 3\\\\ 2\\\\ 3\\\\ 1\\\\ 0 \\end{bmatrix} \\to\n","\\begin{bmatrix} 0\\\\ 0\\\\ 0\\\\ 0\\\\ 1 \\end{bmatrix},\n","\\begin{bmatrix} 0\\\\ 0\\\\ 0\\\\ 1\\\\ 0 \\end{bmatrix},\n","\\begin{bmatrix} 0\\\\ 1\\\\ 0\\\\ 0\\\\ 0 \\end{bmatrix},\n","\\begin{bmatrix} 1\\\\ 0\\\\ 1\\\\ 0\\\\ 0 \\end{bmatrix}\n","\\quad \\text{or in matrix form:} \\quad\n","\\begin{bmatrix}\n","0 & 0 & 0 & 1 \\\\\n","0 & 0 & 1 & 0 \\\\\n","0 & 0 & 0 & 1 \\\\\n","0 & 1 & 0 & 0 \\\\\n","1 & 0 & 0 & 0\n","\\end{bmatrix}\n","$$\n","\n","These new one hot encoded vectors will become the new $b$ in the least squares formulation (formerly `y_train`)."],"metadata":{"id":"nrXAjqSBvWE_"}},{"cell_type":"code","source":["import numpy as np\n","\n","\n","labels = y_train #vector with values ex: labels = [0,5,4,1,3,2,0,2,1,1,4,3,5] except (60000,1)\n","print(labels)\n","n = len(labels)\n","num_categories = max(labels) + 1\n","\n","print(n) #6000\n","print(num_categories) #10\n","\n","#creating one_hot\n","one_hot = np.zeros((n, num_categories))\n","\n","for i, label in enumerate (labels):\n","  one_hot[i, label] = 1\n","\n","#testing how to pull one row or column from one_hot\n","print(\"0th row:\", one_hot[0,:]) #for first row do [1,:]\n","print(\"0th col:\", one_hot[:,0]) #for first col do [:,1]\n","\n","print(\"\\n one_hot:\\n\",one_hot)\n"],"metadata":{"id":"Akq-mvDUy3Fn","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668231190110,"user_tz":480,"elapsed":203,"user":{"displayName":"Kira Burbank","userId":"10437625992165419595"}},"outputId":"3bfd9828-a9fa-4cd8-d441-de9ef41240db"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[5 0 4 ... 5 6 8]\n","60000\n","10\n","0th row: [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n","0th col: [0. 1. 0. ... 0. 0. 0.]\n","\n"," one_hot:\n"," [[0. 0. 0. ... 0. 0. 0.]\n"," [1. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," ...\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 0. 0.]\n"," [0. 0. 0. ... 0. 1. 0.]]\n"]}]},{"cell_type":"markdown","source":["## Homework 3\n","\n","After creating the one hot encoding of the labels, use the $Q$ and $R$ from the code above to solve the least squares problem for $x$ for each of these 10 one hot encoded column vectors.\n","\n","Concat each $x$ into a matrix, $X = \\begin{bmatrix} x_1, & x_2, & \\dots, & x_{10} \\end{bmatrix}$\n","\n","And get the resulting prediction matrix, $Y = A X$.\n","\n","For each row (observation) of this prediction matrix, you will have 10 values that correspond to each label. Whichever value is highest is the prediction. Extract the index of the highest value for each row in this matrix. Compare the predicted labels with the actual labels to calculate your prediction accuracy.\n","\n","### Bonus / Extra Credit\n","\n","Create a confusion matrix of the result and make a short comment about your observations of this matrix."],"metadata":{"id":"nyq8Gctty7C2"}},{"cell_type":"code","source":["#creating empty matrix X where columns x1, x2, x3, ... xnum_categories will be stored\n","\n","#num_categories = length of rows aka how many columns you want X to have\n","\n","X = np.zeros((785, num_categories)) #785x10\n","\n","#getting cols (x1, x2, x3, ... xnum_categories) to go inside X\n","for i in range(num_categories):\n","\n","  #turning cols from one_hot to cols of X\n","\n","  c = Q.T[:rank,:] @ one_hot[:,i]\n","  u = scipy.linalg.solve_triangular(R, c, lower=False)\n","  v = np.zeros(785 - rank)\n","  uv = np.concatenate((u,v))\n","  x = np.eye(785)[:,p] @ uv\n","\n","  X[:,i] = x\n","\n","\n","#Y=AX equivalent to Y=x_train @ X\n","Y = x_train @ X\n","\n","#finding index of max value in pred\n","pred = np.argmax(Y, axis=1)\n","\n","#print first 10 labels of both pred and y_train\n","print(\"pred[:10]\",pred[:10])\n","print(\"y_train[:10]\",y_train[:10])\n","\n","#checking to see how many labels are equal\n","Equal = np.sum(pred == y_train)\n","print(\"labels that are the same in pred and y_train:\", Equal)\n","print(\"amount of labels in y_train:\", len(y_train))\n","\n","#proportion of equal to original labels\n","proportion = Equal / n    #n = 60000\n","percent_proportion = proportion * 100\n","print(\"proportion of correct predicted labels / original labels:\", Equal, \"/\", n, \"=\", proportion)\n","print(\"proportion as a percentage %:\", percent_proportion)"],"metadata":{"id":"vlNvPV0c12LZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668231197899,"user_tz":480,"elapsed":735,"user":{"displayName":"Kira Burbank","userId":"10437625992165419595"}},"outputId":"c53b3099-8f99-4401-c84c-fc9211b2f617"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["pred[:10] [5 0 4 1 9 2 1 3 1 4]\n","y_train[:10] [5 0 4 1 9 2 1 3 1 4]\n","labels that are the same in pred and y_train: 51464\n","amount of labels in y_train: 60000\n","proportion of correct predicted labels / original labels: 51464 / 60000 = 0.8577333333333333\n","proportion as a percentage %: 85.77333333333334\n"]}]},{"cell_type":"markdown","source":["## A note about least squares in practice\n","\n","Please don't implement least squares on your own like this in practice, `numpy` has least squares implemented for you."],"metadata":{"id":"UPV94H8T149m"}},{"cell_type":"code","source":["result = np.linalg.lstsq(x_train, y_train, rcond=1e-6)\n","pred = x_train @ result[0]\n","\n","print(pred[:10])\n","print(y_train[:10])"],"metadata":{"id":"oXRPs-_hqTQR","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1668231207973,"user_tz":480,"elapsed":5820,"user":{"displayName":"Kira Burbank","userId":"10437625992165419595"}},"outputId":"42c6c034-16c5-424b-8799-99feeecd82c3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[4.19480069 1.20585109 3.19619527 2.3125187  7.70189204 4.07685415\n"," 1.62278595 3.92588536 1.84626138 4.82906635]\n","[5 0 4 1 9 2 1 3 1 4]\n"]}]},{"cell_type":"markdown","source":["QR decomposition code aka what scipy.linalg.qr() is doing:"],"metadata":{"id":"Hga966iC-k3d"}},{"cell_type":"code","source":["import numpy as np\n","\n","#gram schmidt\n","\n","A = np.random.rand(4,4)\n","print(\"A:\\n\", A)\n","\n","Q = np.zeros((4,4))\n","Q[:,0] = A[:,0]\n","n = 4\n","\n","#project v onto u\n","def proj(u, v):\n","  angle = u.T @ v\n","  inner_u = u.T @ u\n","  return (angle / inner_u) * u\n","\n","for j in range(n):\n","  v = A[:,j].copy()\n","  for i in range(j):\n","    u = Q[:,i]\n","    projection = proj(u, v)\n","    v -= projection\n","  v /= (v.T @ v) ** 0.5\n","  Q[:,j] = v[:]\n","\n","print(\"Q:\\n\", Q)\n","eye = Q.T @ Q\n","eye[abs(eye) < 1e-10] = 0.0\n","print(\"hopefully indentity:\\n\", eye)\n","\n","R = Q.T @ A\n","R[abs(R) < 1e-10] = 0.0\n","print(\"R:\\n\", R)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YBA0-iGE_InV","executionInfo":{"status":"ok","timestamp":1668231210392,"user_tz":480,"elapsed":160,"user":{"displayName":"Kira Burbank","userId":"10437625992165419595"}},"outputId":"8eaa4dee-5f87-476d-c22f-6612f3288d94"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["A:\n"," [[0.55260458 0.97908672 0.24273866 0.15001341]\n"," [0.31734006 0.77715622 0.00967177 0.77849668]\n"," [0.37092713 0.39320829 0.86332207 0.99480554]\n"," [0.09192235 0.7434321  0.93948786 0.79528849]]\n","Q:\n"," [[ 0.74370452 -0.03520303 -0.29236892 -0.60015394]\n"," [ 0.42708158  0.29946255 -0.44453284  0.72822665]\n"," [ 0.49919996 -0.41632358  0.69660626  0.30366724]\n"," [ 0.12371064  0.85776314  0.48130103 -0.13148154]]\n","hopefully indentity:\n"," [[1. 0. 0. 0.]\n"," [0. 1. 0. 0.]\n"," [0. 0. 1. 0.]\n"," [0. 0. 0. 1.]]\n","R:\n"," [[0.7430432  1.34832034 0.73185145 1.03903977]\n"," [0.         0.67224913 0.44078791 0.49585782]\n"," [0.         0.         0.97830338 0.68583434]\n"," [0.         0.         0.         0.674415  ]]\n"]}]}]}